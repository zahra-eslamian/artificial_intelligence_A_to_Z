{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcIbXqXXtyg1SoTiWqiDIE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahra-eslamian/artificial_intelligence_A_to_Z/blob/main/dqn_lunar_lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System dependency needed to build Box2D / box2d-py\n",
        "!apt-get install -y swig\n",
        "\n",
        "# Gymnasium with Box2D support\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "# (Optional) Only if you really need Atari, not needed for LunarLander:\n",
        "# !pip install \"gymnasium[atari]\"\n",
        "\n",
        "# DQN dependencies\n",
        "!pip install torch\n",
        "!pip install imageio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0xpvgBPvIa0",
        "outputId": "6e414ef6-b900-470f-d5b3-3b7394802d00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 727 kB in 1s (564 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2381959 sha256=fd538b5b03df0a505c775d2497ea82fb6c972e3a7999f08d908ed5c6022a1303\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.4.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import gymnasium as gym\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# =====================\n",
        "# Hyperparameters\n",
        "# =====================\n",
        "ENV_ID = \"LunarLander-v3\"\n",
        "\n",
        "GAMMA = 0.99\n",
        "LR = 5e-4                 # a bit smaller, more stable\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 100_000\n",
        "MIN_REPLAY_SIZE = 5_000   # start learning earlier\n",
        "\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 100_000       # decay over ~100k steps\n",
        "\n",
        "INTERPOLATION_PARAMETER = 1e-3  # tau for soft target network updates\n",
        "\n",
        "MAX_EPISODES = 1000\n",
        "MAX_STEPS_PER_EPISODE = 1000\n",
        "\n",
        "VIDEO_FILENAME = \"lunar_lander_dqn.mp4\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# =====================\n",
        "# DQN Network\n",
        "# =====================\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Replay Buffer\n",
        "# =====================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Epsilon-greedy policy\n",
        "# =====================\n",
        "def get_epsilon(step):\n",
        "    # Linear decay from EPS_START to EPS_END\n",
        "    epsilon = EPS_END + (EPS_START - EPS_END) * max(0, (EPS_DECAY - step)) / EPS_DECAY\n",
        "    return epsilon\n",
        "\n",
        "\n",
        "def select_action(policy_net, state, step, n_actions):\n",
        "    epsilon = get_epsilon(step)\n",
        "    if random.random() < epsilon:\n",
        "        # Explore\n",
        "        return random.randrange(n_actions)\n",
        "    else:\n",
        "        # Exploit\n",
        "        state_v = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = policy_net(state_v)\n",
        "        return int(torch.argmax(q_values, dim=1).item())\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Soft target update (Polyak averaging)\n",
        "# =====================\n",
        "def soft_update(target_net, policy_net, tau):\n",
        "    with torch.no_grad():\n",
        "        for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
        "            target_param.data.copy_(\n",
        "                tau * policy_param.data + (1.0 - tau) * target_param.data\n",
        "            )\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Optimize DQN\n",
        "# =====================\n",
        "def compute_loss(policy_net, target_net, optimizer, replay_buffer):\n",
        "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    # Current Q values\n",
        "    q_values = policy_net(states)\n",
        "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Target Q values\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_net(next_states).max(1)[0]\n",
        "        target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
        "\n",
        "    # Huber loss (SmoothL1Loss) instead of MSE\n",
        "    loss = nn.SmoothL1Loss()(q_values, target_q_values)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # Gradient clipping for stability\n",
        "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Evaluation of trained agent\n",
        "# =====================\n",
        "def evaluate_agent(policy_net, n_episodes=20):\n",
        "    env = gym.make(ENV_ID)\n",
        "    rewards = []\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        state, _ = env.reset(seed=ep)\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            state_v = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = policy_net(state_v)\n",
        "                action = int(torch.argmax(q_values, dim=1).item())\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            ep_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "\n",
        "    env.close()\n",
        "    print(f\"\\nEvaluation over {n_episodes} episodes:\")\n",
        "    print(f\"  Mean reward: {np.mean(rewards):.2f}\")\n",
        "    print(f\"  Min reward:  {np.min(rewards):.2f}\")\n",
        "    print(f\"  Max reward:  {np.max(rewards):.2f}\\n\")\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Record best episode video\n",
        "# =====================\n",
        "def record_best_video(policy_net, filename=VIDEO_FILENAME, episodes=10):\n",
        "    \"\"\"\n",
        "    Run the trained policy for several episodes and save a video\n",
        "    of the *best* episode (highest total reward).\n",
        "    \"\"\"\n",
        "    video_env = gym.make(ENV_ID, render_mode=\"rgb_array\")\n",
        "\n",
        "    best_frames = None\n",
        "    best_reward = -float(\"inf\")\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = video_env.reset(seed=ep)\n",
        "        done = False\n",
        "        step = 0\n",
        "        ep_reward = 0.0\n",
        "        frames = []\n",
        "\n",
        "        while not done and step < MAX_STEPS_PER_EPISODE:\n",
        "            state_v = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = policy_net(state_v)\n",
        "                action = int(torch.argmax(q_values, dim=1).item())\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = video_env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            frame = video_env.render()\n",
        "            frames.append(frame)\n",
        "\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            step += 1\n",
        "\n",
        "        print(f\"Video rollout episode {ep}: total reward = {ep_reward:.2f}\")\n",
        "\n",
        "        if ep_reward > best_reward:\n",
        "            best_reward = ep_reward\n",
        "            best_frames = frames\n",
        "\n",
        "    video_env.close()\n",
        "\n",
        "    if best_frames is None:\n",
        "        print(\"No episodes recorded, something went wrong.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nSaving best episode (reward={best_reward:.2f}) \"\n",
        "          f\"to {filename} with {len(best_frames)} frames...\")\n",
        "    with imageio.get_writer(filename, fps=30) as writer:\n",
        "        for frame in best_frames:\n",
        "            writer.append_data(frame)\n",
        "    print(\"Video saved:\", filename)\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Main training loop\n",
        "# =====================\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(ENV_ID)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    print(\"Observation dim:\", obs_dim)\n",
        "    print(\"Number of actions:\", n_actions)\n",
        "\n",
        "    # Networks\n",
        "    policy_net = DQN(obs_dim, n_actions).to(device)\n",
        "    target_net = DQN(obs_dim, n_actions).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "    replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
        "\n",
        "    # Fill replay buffer with random experience\n",
        "    print(\"Filling replay buffer with random policy...\")\n",
        "    state, _ = env.reset(seed=0)\n",
        "    while len(replay_buffer) < MIN_REPLAY_SIZE:\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "        if done:\n",
        "            state, _ = env.reset()\n",
        "        else:\n",
        "            state = next_state\n",
        "\n",
        "    print(f\"Replay buffer filled with {len(replay_buffer)} transitions. Starting training.\")\n",
        "\n",
        "    total_steps = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(1, MAX_EPISODES + 1):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        for t in range(MAX_STEPS_PER_EPISODE):\n",
        "            total_steps += 1\n",
        "\n",
        "            # Choose action\n",
        "            action = select_action(policy_net, state, total_steps, n_actions)\n",
        "\n",
        "            # Environment step\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Store experience\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Optimize model\n",
        "            loss = compute_loss(policy_net, target_net, optimizer, replay_buffer)\n",
        "\n",
        "            # Soft update target network\n",
        "            soft_update(target_net, policy_net, INTERPOLATION_PARAMETER)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        avg_reward = np.mean(episode_rewards[-20:])\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode:4d} | \"\n",
        "            f\"Reward: {episode_reward:7.2f} | \"\n",
        "            f\"Avg(20): {avg_reward:7.2f} | \"\n",
        "            f\"Steps: {total_steps:7d} | \"\n",
        "            f\"Epsilon: {get_epsilon(total_steps):.3f}\"\n",
        "        )\n",
        "\n",
        "        # Simple \"solved\" condition\n",
        "        if avg_reward >= 200.0 and episode >= 20:\n",
        "            print(\"Environment solved, stopping training.\")\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Evaluate trained agent\n",
        "    evaluate_agent(policy_net, n_episodes=20)\n",
        "\n",
        "    # Record video of the best episode among several rollouts\n",
        "    record_best_video(policy_net, filename=VIDEO_FILENAME, episodes=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kd_ueGThFFA",
        "outputId": "2723aa18-387a-4ad0-d215-6f7e88e2285f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Observation dim: 8\n",
            "Number of actions: 4\n",
            "Filling replay buffer with random policy...\n",
            "Replay buffer filled with 5000 transitions. Starting training.\n",
            "Episode    1 | Reward: -103.66 | Avg(20): -103.66 | Steps:      73 | Epsilon: 0.999\n",
            "Episode    2 | Reward: -187.40 | Avg(20): -145.53 | Steps:     172 | Epsilon: 0.998\n",
            "Episode    3 | Reward: -202.66 | Avg(20): -164.57 | Steps:     263 | Epsilon: 0.998\n",
            "Episode    4 | Reward:  -83.26 | Avg(20): -144.24 | Steps:     365 | Epsilon: 0.997\n",
            "Episode    5 | Reward: -140.18 | Avg(20): -143.43 | Steps:     435 | Epsilon: 0.996\n",
            "Episode    6 | Reward:  -71.93 | Avg(20): -131.51 | Steps:     506 | Epsilon: 0.995\n",
            "Episode    7 | Reward: -148.75 | Avg(20): -133.98 | Steps:     604 | Epsilon: 0.994\n",
            "Episode    8 | Reward: -323.09 | Avg(20): -157.62 | Steps:     726 | Epsilon: 0.993\n",
            "Episode    9 | Reward: -267.25 | Avg(20): -169.80 | Steps:     817 | Epsilon: 0.992\n",
            "Episode   10 | Reward: -110.44 | Avg(20): -163.86 | Steps:     879 | Epsilon: 0.992\n",
            "Episode   11 | Reward: -459.71 | Avg(20): -190.76 | Steps:     965 | Epsilon: 0.991\n",
            "Episode   12 | Reward: -274.66 | Avg(20): -197.75 | Steps:    1051 | Epsilon: 0.990\n",
            "Episode   13 | Reward: -343.52 | Avg(20): -208.96 | Steps:    1136 | Epsilon: 0.989\n",
            "Episode   14 | Reward: -112.98 | Avg(20): -202.11 | Steps:    1221 | Epsilon: 0.988\n",
            "Episode   15 | Reward: -348.50 | Avg(20): -211.87 | Steps:    1352 | Epsilon: 0.987\n",
            "Episode   16 | Reward: -316.91 | Avg(20): -218.43 | Steps:    1440 | Epsilon: 0.986\n",
            "Episode   17 | Reward: -274.77 | Avg(20): -221.75 | Steps:    1571 | Epsilon: 0.985\n",
            "Episode   18 | Reward: -108.16 | Avg(20): -215.44 | Steps:    1637 | Epsilon: 0.984\n",
            "Episode   19 | Reward:  -74.95 | Avg(20): -208.04 | Steps:    1715 | Epsilon: 0.984\n",
            "Episode   20 | Reward: -178.04 | Avg(20): -206.54 | Steps:    1829 | Epsilon: 0.983\n",
            "Episode   21 | Reward: -323.47 | Avg(20): -217.53 | Steps:    1925 | Epsilon: 0.982\n",
            "Episode   22 | Reward: -136.67 | Avg(20): -215.00 | Steps:    1988 | Epsilon: 0.981\n",
            "Episode   23 | Reward:  -58.03 | Avg(20): -207.76 | Steps:    2076 | Epsilon: 0.980\n",
            "Episode   24 | Reward: -307.78 | Avg(20): -218.99 | Steps:    2169 | Epsilon: 0.979\n",
            "Episode   25 | Reward:  -79.46 | Avg(20): -215.95 | Steps:    2282 | Epsilon: 0.978\n",
            "Episode   26 | Reward: -106.58 | Avg(20): -217.69 | Steps:    2349 | Epsilon: 0.978\n",
            "Episode   27 | Reward: -164.16 | Avg(20): -218.46 | Steps:    2421 | Epsilon: 0.977\n",
            "Episode   28 | Reward:  -99.20 | Avg(20): -207.26 | Steps:    2493 | Epsilon: 0.976\n",
            "Episode   29 | Reward: -169.12 | Avg(20): -202.36 | Steps:    2624 | Epsilon: 0.975\n",
            "Episode   30 | Reward: -186.54 | Avg(20): -206.16 | Steps:    2713 | Epsilon: 0.974\n",
            "Episode   31 | Reward: -118.54 | Avg(20): -189.10 | Steps:    2825 | Epsilon: 0.973\n",
            "Episode   32 | Reward: -178.42 | Avg(20): -184.29 | Steps:    2919 | Epsilon: 0.972\n",
            "Episode   33 | Reward: -195.58 | Avg(20): -176.89 | Steps:    3009 | Epsilon: 0.971\n",
            "Episode   34 | Reward: -112.45 | Avg(20): -176.87 | Steps:    3102 | Epsilon: 0.971\n",
            "Episode   35 | Reward: -104.77 | Avg(20): -164.68 | Steps:    3166 | Epsilon: 0.970\n",
            "Episode   36 | Reward: -431.63 | Avg(20): -170.42 | Steps:    3309 | Epsilon: 0.969\n",
            "Episode   37 | Reward:  -65.26 | Avg(20): -159.94 | Steps:    3407 | Epsilon: 0.968\n",
            "Episode   38 | Reward: -106.73 | Avg(20): -159.87 | Steps:    3514 | Epsilon: 0.967\n",
            "Episode   39 | Reward: -107.24 | Avg(20): -161.48 | Steps:    3600 | Epsilon: 0.966\n",
            "Episode   40 | Reward:  -68.78 | Avg(20): -156.02 | Steps:    3674 | Epsilon: 0.965\n",
            "Episode   41 | Reward: -444.62 | Avg(20): -162.08 | Steps:    3800 | Epsilon: 0.964\n",
            "Episode   42 | Reward:  -81.66 | Avg(20): -159.33 | Steps:    3870 | Epsilon: 0.963\n",
            "Episode   43 | Reward:  -50.51 | Avg(20): -158.95 | Steps:    3933 | Epsilon: 0.963\n",
            "Episode   44 | Reward:  -98.92 | Avg(20): -148.51 | Steps:    4028 | Epsilon: 0.962\n",
            "Episode   45 | Reward: -259.97 | Avg(20): -157.53 | Steps:    4121 | Epsilon: 0.961\n",
            "Episode   46 | Reward: -132.86 | Avg(20): -158.85 | Steps:    4196 | Epsilon: 0.960\n",
            "Episode   47 | Reward: -148.04 | Avg(20): -158.04 | Steps:    4257 | Epsilon: 0.960\n",
            "Episode   48 | Reward:  -82.60 | Avg(20): -157.21 | Steps:    4374 | Epsilon: 0.958\n",
            "Episode   49 | Reward: -145.14 | Avg(20): -156.01 | Steps:    4463 | Epsilon: 0.958\n",
            "Episode   50 | Reward:  -53.80 | Avg(20): -149.37 | Steps:    4555 | Epsilon: 0.957\n",
            "Episode   51 | Reward: -196.93 | Avg(20): -153.29 | Steps:    4639 | Epsilon: 0.956\n",
            "Episode   52 | Reward: -105.19 | Avg(20): -149.63 | Steps:    4738 | Epsilon: 0.955\n",
            "Episode   53 | Reward: -201.22 | Avg(20): -149.91 | Steps:    4809 | Epsilon: 0.954\n",
            "Episode   54 | Reward: -211.84 | Avg(20): -154.88 | Steps:    4950 | Epsilon: 0.953\n",
            "Episode   55 | Reward: -168.54 | Avg(20): -158.07 | Steps:    5028 | Epsilon: 0.952\n",
            "Episode   56 | Reward:  -64.80 | Avg(20): -139.73 | Steps:    5104 | Epsilon: 0.952\n",
            "Episode   57 | Reward: -103.31 | Avg(20): -141.63 | Steps:    5191 | Epsilon: 0.951\n",
            "Episode   58 | Reward:  -90.98 | Avg(20): -140.85 | Steps:    5259 | Epsilon: 0.950\n",
            "Episode   59 | Reward: -201.68 | Avg(20): -145.57 | Steps:    5385 | Epsilon: 0.949\n",
            "Episode   60 | Reward:    2.21 | Avg(20): -142.02 | Steps:    5517 | Epsilon: 0.948\n",
            "Episode   61 | Reward: -145.89 | Avg(20): -127.08 | Steps:    5605 | Epsilon: 0.947\n",
            "Episode   62 | Reward: -310.96 | Avg(20): -138.55 | Steps:    5733 | Epsilon: 0.946\n",
            "Episode   63 | Reward: -233.75 | Avg(20): -147.71 | Steps:    5819 | Epsilon: 0.945\n",
            "Episode   64 | Reward: -123.40 | Avg(20): -148.93 | Steps:    5898 | Epsilon: 0.944\n",
            "Episode   65 | Reward: -161.51 | Avg(20): -144.01 | Steps:    6006 | Epsilon: 0.943\n",
            "Episode   66 | Reward: -103.40 | Avg(20): -142.54 | Steps:    6099 | Epsilon: 0.942\n",
            "Episode   67 | Reward:  -92.98 | Avg(20): -139.79 | Steps:    6198 | Epsilon: 0.941\n",
            "Episode   68 | Reward: -134.56 | Avg(20): -142.38 | Steps:    6288 | Epsilon: 0.940\n",
            "Episode   69 | Reward: -446.99 | Avg(20): -157.48 | Steps:    6426 | Epsilon: 0.939\n",
            "Episode   70 | Reward: -139.78 | Avg(20): -161.77 | Steps:    6492 | Epsilon: 0.938\n",
            "Episode   71 | Reward:  -91.04 | Avg(20): -156.48 | Steps:    6571 | Epsilon: 0.938\n",
            "Episode   72 | Reward:  -43.50 | Avg(20): -153.40 | Steps:    6650 | Epsilon: 0.937\n",
            "Episode   73 | Reward:  -71.94 | Avg(20): -146.93 | Steps:    6718 | Epsilon: 0.936\n",
            "Episode   74 | Reward:  -85.33 | Avg(20): -140.61 | Steps:    6823 | Epsilon: 0.935\n",
            "Episode   75 | Reward:  -63.01 | Avg(20): -135.33 | Steps:    6884 | Epsilon: 0.935\n",
            "Episode   76 | Reward: -181.35 | Avg(20): -141.16 | Steps:    6954 | Epsilon: 0.934\n",
            "Episode   77 | Reward: -111.05 | Avg(20): -141.54 | Steps:    7034 | Epsilon: 0.933\n",
            "Episode   78 | Reward: -127.41 | Avg(20): -143.37 | Steps:    7137 | Epsilon: 0.932\n",
            "Episode   79 | Reward: -224.27 | Avg(20): -144.49 | Steps:    7249 | Epsilon: 0.931\n",
            "Episode   80 | Reward: -115.01 | Avg(20): -150.36 | Steps:    7334 | Epsilon: 0.930\n",
            "Episode   81 | Reward:    7.49 | Avg(20): -142.69 | Steps:    7456 | Epsilon: 0.929\n",
            "Episode   82 | Reward: -289.04 | Avg(20): -141.59 | Steps:    7570 | Epsilon: 0.928\n",
            "Episode   83 | Reward: -103.62 | Avg(20): -135.08 | Steps:    7684 | Epsilon: 0.927\n",
            "Episode   84 | Reward:  -87.95 | Avg(20): -133.31 | Steps:    7770 | Epsilon: 0.926\n",
            "Episode   85 | Reward: -162.98 | Avg(20): -133.39 | Steps:    7841 | Epsilon: 0.926\n",
            "Episode   86 | Reward:  -86.98 | Avg(20): -132.56 | Steps:    7908 | Epsilon: 0.925\n",
            "Episode   87 | Reward: -272.61 | Avg(20): -141.55 | Steps:    7995 | Epsilon: 0.924\n",
            "Episode   88 | Reward:  -44.00 | Avg(20): -137.02 | Steps:    8128 | Epsilon: 0.923\n",
            "Episode   89 | Reward:  -76.96 | Avg(20): -118.52 | Steps:    8209 | Epsilon: 0.922\n",
            "Episode   90 | Reward: -101.56 | Avg(20): -116.61 | Steps:    8287 | Epsilon: 0.921\n",
            "Episode   91 | Reward: -100.26 | Avg(20): -117.07 | Steps:    8369 | Epsilon: 0.920\n",
            "Episode   92 | Reward: -112.60 | Avg(20): -120.52 | Steps:    8458 | Epsilon: 0.920\n",
            "Episode   93 | Reward: -129.15 | Avg(20): -123.38 | Steps:    8566 | Epsilon: 0.919\n",
            "Episode   94 | Reward:  -59.55 | Avg(20): -122.09 | Steps:    8654 | Epsilon: 0.918\n",
            "Episode   95 | Reward:  -99.26 | Avg(20): -123.91 | Steps:    8730 | Epsilon: 0.917\n",
            "Episode   96 | Reward:  -85.53 | Avg(20): -119.11 | Steps:    8790 | Epsilon: 0.916\n",
            "Episode   97 | Reward: -195.91 | Avg(20): -123.36 | Steps:    8888 | Epsilon: 0.916\n",
            "Episode   98 | Reward: -100.62 | Avg(20): -122.02 | Steps:    8966 | Epsilon: 0.915\n",
            "Episode   99 | Reward:  -57.15 | Avg(20): -113.66 | Steps:    9026 | Epsilon: 0.914\n",
            "Episode  100 | Reward:  -61.57 | Avg(20): -110.99 | Steps:    9103 | Epsilon: 0.914\n",
            "Episode  101 | Reward: -292.58 | Avg(20): -125.99 | Steps:    9212 | Epsilon: 0.912\n",
            "Episode  102 | Reward: -147.31 | Avg(20): -118.91 | Steps:    9274 | Epsilon: 0.912\n",
            "Episode  103 | Reward: -140.72 | Avg(20): -120.76 | Steps:    9347 | Epsilon: 0.911\n",
            "Episode  104 | Reward:  -68.76 | Avg(20): -119.80 | Steps:    9423 | Epsilon: 0.910\n",
            "Episode  105 | Reward: -166.95 | Avg(20): -120.00 | Steps:    9524 | Epsilon: 0.910\n",
            "Episode  106 | Reward: -108.74 | Avg(20): -121.09 | Steps:    9607 | Epsilon: 0.909\n",
            "Episode  107 | Reward: -235.02 | Avg(20): -119.21 | Steps:    9692 | Epsilon: 0.908\n",
            "Episode  108 | Reward: -108.47 | Avg(20): -122.43 | Steps:    9805 | Epsilon: 0.907\n",
            "Episode  109 | Reward: -216.03 | Avg(20): -129.39 | Steps:    9888 | Epsilon: 0.906\n",
            "Episode  110 | Reward:  -54.08 | Avg(20): -127.01 | Steps:   10019 | Epsilon: 0.905\n",
            "Episode  111 | Reward: -100.98 | Avg(20): -127.05 | Steps:   10076 | Epsilon: 0.904\n",
            "Episode  112 | Reward: -116.04 | Avg(20): -127.22 | Steps:   10176 | Epsilon: 0.903\n",
            "Episode  113 | Reward:  -93.69 | Avg(20): -125.45 | Steps:   10309 | Epsilon: 0.902\n",
            "Episode  114 | Reward: -190.76 | Avg(20): -132.01 | Steps:   10443 | Epsilon: 0.901\n",
            "Episode  115 | Reward:  -66.55 | Avg(20): -130.37 | Steps:   10509 | Epsilon: 0.900\n",
            "Episode  116 | Reward: -132.67 | Avg(20): -132.73 | Steps:   10627 | Epsilon: 0.899\n",
            "Episode  117 | Reward: -186.68 | Avg(20): -132.27 | Steps:   10719 | Epsilon: 0.898\n",
            "Episode  118 | Reward: -129.95 | Avg(20): -133.74 | Steps:   10815 | Epsilon: 0.897\n",
            "Episode  119 | Reward: -250.51 | Avg(20): -143.40 | Steps:   10896 | Epsilon: 0.896\n",
            "Episode  120 | Reward:  -95.92 | Avg(20): -145.12 | Steps:   10991 | Epsilon: 0.896\n",
            "Episode  121 | Reward:  -91.07 | Avg(20): -135.05 | Steps:   11118 | Epsilon: 0.894\n",
            "Episode  122 | Reward:  -30.82 | Avg(20): -129.22 | Steps:   11229 | Epsilon: 0.893\n",
            "Episode  123 | Reward:  -85.91 | Avg(20): -126.48 | Steps:   11289 | Epsilon: 0.893\n",
            "Episode  124 | Reward: -100.97 | Avg(20): -128.09 | Steps:   11407 | Epsilon: 0.892\n",
            "Episode  125 | Reward: -106.86 | Avg(20): -125.09 | Steps:   11505 | Epsilon: 0.891\n",
            "Episode  126 | Reward: -117.51 | Avg(20): -125.53 | Steps:   11613 | Epsilon: 0.890\n",
            "Episode  127 | Reward: -133.55 | Avg(20): -120.45 | Steps:   11710 | Epsilon: 0.889\n",
            "Episode  128 | Reward:  -98.05 | Avg(20): -119.93 | Steps:   11840 | Epsilon: 0.888\n",
            "Episode  129 | Reward: -107.31 | Avg(20): -114.49 | Steps:   11957 | Epsilon: 0.886\n",
            "Episode  130 | Reward: -101.01 | Avg(20): -116.84 | Steps:   12080 | Epsilon: 0.885\n",
            "Episode  131 | Reward:  -76.39 | Avg(20): -115.61 | Steps:   12170 | Epsilon: 0.884\n",
            "Episode  132 | Reward: -106.18 | Avg(20): -115.12 | Steps:   12234 | Epsilon: 0.884\n",
            "Episode  133 | Reward: -134.21 | Avg(20): -117.14 | Steps:   12350 | Epsilon: 0.883\n",
            "Episode  134 | Reward:  -71.73 | Avg(20): -111.19 | Steps:   12431 | Epsilon: 0.882\n",
            "Episode  135 | Reward:  -65.23 | Avg(20): -111.13 | Steps:   12501 | Epsilon: 0.881\n",
            "Episode  136 | Reward:  -76.03 | Avg(20): -108.29 | Steps:   12572 | Epsilon: 0.881\n",
            "Episode  137 | Reward: -105.15 | Avg(20): -104.22 | Steps:   12679 | Epsilon: 0.880\n",
            "Episode  138 | Reward: -102.72 | Avg(20): -102.86 | Steps:   12793 | Epsilon: 0.878\n",
            "Episode  139 | Reward: -130.42 | Avg(20):  -96.85 | Steps:   12861 | Epsilon: 0.878\n",
            "Episode  140 | Reward:  -79.83 | Avg(20):  -96.05 | Steps:   12959 | Epsilon: 0.877\n",
            "Episode  141 | Reward:  -87.05 | Avg(20):  -95.85 | Steps:   13029 | Epsilon: 0.876\n",
            "Episode  142 | Reward:  -98.65 | Avg(20):  -99.24 | Steps:   13089 | Epsilon: 0.876\n",
            "Episode  143 | Reward:  -99.96 | Avg(20):  -99.94 | Steps:   13197 | Epsilon: 0.875\n",
            "Episode  144 | Reward:  -89.77 | Avg(20):  -99.38 | Steps:   13280 | Epsilon: 0.874\n",
            "Episode  145 | Reward: -128.26 | Avg(20): -100.45 | Steps:   13376 | Epsilon: 0.873\n",
            "Episode  146 | Reward: -102.90 | Avg(20):  -99.72 | Steps:   13496 | Epsilon: 0.872\n",
            "Episode  147 | Reward: -404.24 | Avg(20): -113.25 | Steps:   13665 | Epsilon: 0.870\n",
            "Episode  148 | Reward:  -55.46 | Avg(20): -111.13 | Steps:   13764 | Epsilon: 0.869\n",
            "Episode  149 | Reward:  -72.64 | Avg(20): -109.39 | Steps:   13877 | Epsilon: 0.868\n",
            "Episode  150 | Reward: -128.54 | Avg(20): -110.77 | Steps:   13975 | Epsilon: 0.867\n",
            "Episode  151 | Reward:  -90.57 | Avg(20): -111.48 | Steps:   14083 | Epsilon: 0.866\n",
            "Episode  152 | Reward: -148.87 | Avg(20): -113.61 | Steps:   14188 | Epsilon: 0.865\n",
            "Episode  153 | Reward:  -69.06 | Avg(20): -110.35 | Steps:   14258 | Epsilon: 0.865\n",
            "Episode  154 | Reward:  -96.97 | Avg(20): -111.62 | Steps:   14344 | Epsilon: 0.864\n",
            "Episode  155 | Reward: -123.32 | Avg(20): -114.52 | Steps:   14405 | Epsilon: 0.863\n",
            "Episode  156 | Reward:  -78.44 | Avg(20): -114.64 | Steps:   14493 | Epsilon: 0.862\n",
            "Episode  157 | Reward:  -78.63 | Avg(20): -113.32 | Steps:   14569 | Epsilon: 0.862\n",
            "Episode  158 | Reward:  -63.61 | Avg(20): -111.36 | Steps:   14644 | Epsilon: 0.861\n",
            "Episode  159 | Reward: -113.25 | Avg(20): -110.50 | Steps:   14750 | Epsilon: 0.860\n",
            "Episode  160 | Reward: -146.73 | Avg(20): -113.85 | Steps:   14851 | Epsilon: 0.859\n",
            "Episode  161 | Reward: -143.62 | Avg(20): -116.68 | Steps:   14938 | Epsilon: 0.858\n",
            "Episode  162 | Reward: -147.87 | Avg(20): -119.14 | Steps:   15002 | Epsilon: 0.857\n",
            "Episode  163 | Reward: -217.46 | Avg(20): -125.01 | Steps:   15092 | Epsilon: 0.857\n",
            "Episode  164 | Reward: -208.09 | Avg(20): -130.93 | Steps:   15190 | Epsilon: 0.856\n",
            "Episode  165 | Reward: -156.13 | Avg(20): -132.32 | Steps:   15309 | Epsilon: 0.855\n",
            "Episode  166 | Reward: -208.46 | Avg(20): -137.60 | Steps:   15428 | Epsilon: 0.853\n",
            "Episode  167 | Reward: -108.64 | Avg(20): -122.82 | Steps:   15494 | Epsilon: 0.853\n",
            "Episode  168 | Reward: -107.62 | Avg(20): -125.43 | Steps:   15615 | Epsilon: 0.852\n",
            "Episode  169 | Reward:  -87.21 | Avg(20): -126.16 | Steps:   15689 | Epsilon: 0.851\n",
            "Episode  170 | Reward:  -92.03 | Avg(20): -124.33 | Steps:   15767 | Epsilon: 0.850\n",
            "Episode  171 | Reward: -169.97 | Avg(20): -128.30 | Steps:   15873 | Epsilon: 0.849\n",
            "Episode  172 | Reward: -129.60 | Avg(20): -127.34 | Steps:   15962 | Epsilon: 0.848\n",
            "Episode  173 | Reward:  -65.77 | Avg(20): -127.17 | Steps:   16037 | Epsilon: 0.848\n",
            "Episode  174 | Reward: -107.79 | Avg(20): -127.71 | Steps:   16116 | Epsilon: 0.847\n",
            "Episode  175 | Reward:  -66.99 | Avg(20): -124.90 | Steps:   16202 | Epsilon: 0.846\n",
            "Episode  176 | Reward:  -99.90 | Avg(20): -125.97 | Steps:   16314 | Epsilon: 0.845\n",
            "Episode  177 | Reward:  -66.38 | Avg(20): -125.36 | Steps:   16409 | Epsilon: 0.844\n",
            "Episode  178 | Reward: -119.62 | Avg(20): -128.16 | Steps:   16554 | Epsilon: 0.843\n",
            "Episode  179 | Reward:  -60.90 | Avg(20): -125.54 | Steps:   16624 | Epsilon: 0.842\n",
            "Episode  180 | Reward:    8.97 | Avg(20): -117.75 | Steps:   16722 | Epsilon: 0.841\n",
            "Episode  181 | Reward: -191.79 | Avg(20): -120.16 | Steps:   16806 | Epsilon: 0.840\n",
            "Episode  182 | Reward:  -86.91 | Avg(20): -117.11 | Steps:   16911 | Epsilon: 0.839\n",
            "Episode  183 | Reward:  -48.70 | Avg(20): -108.68 | Steps:   16992 | Epsilon: 0.839\n",
            "Episode  184 | Reward:  -81.53 | Avg(20): -102.35 | Steps:   17108 | Epsilon: 0.837\n",
            "Episode  185 | Reward:  -93.00 | Avg(20):  -99.19 | Steps:   17182 | Epsilon: 0.837\n",
            "Episode  186 | Reward: -118.56 | Avg(20):  -94.70 | Steps:   17276 | Epsilon: 0.836\n",
            "Episode  187 | Reward: -133.14 | Avg(20):  -95.92 | Steps:   17392 | Epsilon: 0.835\n",
            "Episode  188 | Reward:  -72.72 | Avg(20):  -94.18 | Steps:   17481 | Epsilon: 0.834\n",
            "Episode  189 | Reward:  -87.60 | Avg(20):  -94.20 | Steps:   17601 | Epsilon: 0.833\n",
            "Episode  190 | Reward:  -31.63 | Avg(20):  -91.18 | Steps:   17750 | Epsilon: 0.831\n",
            "Episode  191 | Reward: -332.16 | Avg(20):  -99.29 | Steps:   17872 | Epsilon: 0.830\n",
            "Episode  192 | Reward:  -62.22 | Avg(20):  -95.92 | Steps:   17942 | Epsilon: 0.830\n",
            "Episode  193 | Reward: -115.46 | Avg(20):  -98.40 | Steps:   18019 | Epsilon: 0.829\n",
            "Episode  194 | Reward: -216.54 | Avg(20): -103.84 | Steps:   18119 | Epsilon: 0.828\n",
            "Episode  195 | Reward: -173.75 | Avg(20): -109.18 | Steps:   18234 | Epsilon: 0.827\n",
            "Episode  196 | Reward:  -84.43 | Avg(20): -108.40 | Steps:   18344 | Epsilon: 0.826\n",
            "Episode  197 | Reward:  -90.55 | Avg(20): -109.61 | Steps:   18476 | Epsilon: 0.824\n",
            "Episode  198 | Reward: -167.31 | Avg(20): -112.00 | Steps:   18564 | Epsilon: 0.824\n",
            "Episode  199 | Reward:  -30.80 | Avg(20): -110.49 | Steps:   18680 | Epsilon: 0.823\n",
            "Episode  200 | Reward:  -46.00 | Avg(20): -113.24 | Steps:   18786 | Epsilon: 0.822\n",
            "Episode  201 | Reward:  -87.43 | Avg(20): -108.02 | Steps:   18915 | Epsilon: 0.820\n",
            "Episode  202 | Reward:   -7.04 | Avg(20): -104.03 | Steps:   19005 | Epsilon: 0.819\n",
            "Episode  203 | Reward:  -83.86 | Avg(20): -105.79 | Steps:   19096 | Epsilon: 0.819\n",
            "Episode  204 | Reward:  -58.72 | Avg(20): -104.65 | Steps:   19206 | Epsilon: 0.818\n",
            "Episode  205 | Reward:  -47.84 | Avg(20): -102.39 | Steps:   19292 | Epsilon: 0.817\n",
            "Episode  206 | Reward:  -64.52 | Avg(20):  -99.69 | Steps:   19409 | Epsilon: 0.816\n",
            "Episode  207 | Reward: -100.27 | Avg(20):  -98.04 | Steps:   19512 | Epsilon: 0.815\n",
            "Episode  208 | Reward: -222.69 | Avg(20): -105.54 | Steps:   19636 | Epsilon: 0.813\n",
            "Episode  209 | Reward:  -49.96 | Avg(20): -103.66 | Steps:   19776 | Epsilon: 0.812\n",
            "Episode  210 | Reward:  -79.52 | Avg(20): -106.05 | Steps:   19852 | Epsilon: 0.811\n",
            "Episode  211 | Reward:  -78.11 | Avg(20):  -93.35 | Steps:   19957 | Epsilon: 0.810\n",
            "Episode  212 | Reward:  -76.63 | Avg(20):  -94.07 | Steps:   20032 | Epsilon: 0.810\n",
            "Episode  213 | Reward: -120.94 | Avg(20):  -94.34 | Steps:   20121 | Epsilon: 0.809\n",
            "Episode  214 | Reward:  -86.08 | Avg(20):  -87.82 | Steps:   20204 | Epsilon: 0.808\n",
            "Episode  215 | Reward:  -98.95 | Avg(20):  -84.08 | Steps:   20277 | Epsilon: 0.807\n",
            "Episode  216 | Reward:  -50.12 | Avg(20):  -82.37 | Steps:   20352 | Epsilon: 0.807\n",
            "Episode  217 | Reward:  -87.65 | Avg(20):  -82.22 | Steps:   20444 | Epsilon: 0.806\n",
            "Episode  218 | Reward: -106.51 | Avg(20):  -79.18 | Steps:   20552 | Epsilon: 0.805\n",
            "Episode  219 | Reward:  -90.33 | Avg(20):  -82.16 | Steps:   20625 | Epsilon: 0.804\n",
            "Episode  220 | Reward:  -56.97 | Avg(20):  -82.71 | Steps:   20689 | Epsilon: 0.803\n",
            "Episode  221 | Reward:  -58.73 | Avg(20):  -81.27 | Steps:   20762 | Epsilon: 0.803\n",
            "Episode  222 | Reward:  -91.35 | Avg(20):  -85.49 | Steps:   20930 | Epsilon: 0.801\n",
            "Episode  223 | Reward:  -87.20 | Avg(20):  -85.65 | Steps:   21032 | Epsilon: 0.800\n",
            "Episode  224 | Reward: -105.39 | Avg(20):  -87.99 | Steps:   21124 | Epsilon: 0.799\n",
            "Episode  225 | Reward:  -92.08 | Avg(20):  -90.20 | Steps:   21236 | Epsilon: 0.798\n",
            "Episode  226 | Reward:  -92.31 | Avg(20):  -91.59 | Steps:   21325 | Epsilon: 0.797\n",
            "Episode  227 | Reward:  -82.67 | Avg(20):  -90.71 | Steps:   21415 | Epsilon: 0.797\n",
            "Episode  228 | Reward: -202.42 | Avg(20):  -89.70 | Steps:   21503 | Epsilon: 0.796\n",
            "Episode  229 | Reward: -113.36 | Avg(20):  -92.87 | Steps:   21580 | Epsilon: 0.795\n",
            "Episode  230 | Reward: -125.23 | Avg(20):  -95.15 | Steps:   21655 | Epsilon: 0.794\n",
            "Episode  231 | Reward: -163.66 | Avg(20):  -99.43 | Steps:   21753 | Epsilon: 0.793\n",
            "Episode  232 | Reward:  -82.48 | Avg(20):  -99.72 | Steps:   21863 | Epsilon: 0.792\n",
            "Episode  233 | Reward:  -59.26 | Avg(20):  -96.64 | Steps:   21958 | Epsilon: 0.791\n",
            "Episode  234 | Reward:  -66.04 | Avg(20):  -95.64 | Steps:   22024 | Epsilon: 0.791\n",
            "Episode  235 | Reward:  -78.67 | Avg(20):  -94.62 | Steps:   22128 | Epsilon: 0.790\n",
            "Episode  236 | Reward: -133.71 | Avg(20):  -98.80 | Steps:   22235 | Epsilon: 0.789\n",
            "Episode  237 | Reward:  -86.24 | Avg(20):  -98.73 | Steps:   22318 | Epsilon: 0.788\n",
            "Episode  238 | Reward:  -98.59 | Avg(20):  -98.33 | Steps:   22388 | Epsilon: 0.787\n",
            "Episode  239 | Reward:   11.84 | Avg(20):  -93.23 | Steps:   22545 | Epsilon: 0.786\n",
            "Episode  240 | Reward:  -76.29 | Avg(20):  -94.19 | Steps:   22605 | Epsilon: 0.785\n",
            "Episode  241 | Reward:  -35.31 | Avg(20):  -93.02 | Steps:   22695 | Epsilon: 0.784\n",
            "Episode  242 | Reward:  -61.29 | Avg(20):  -91.52 | Steps:   22789 | Epsilon: 0.784\n",
            "Episode  243 | Reward:  -52.80 | Avg(20):  -89.80 | Steps:   22925 | Epsilon: 0.782\n",
            "Episode  244 | Reward:  -72.98 | Avg(20):  -88.18 | Steps:   23036 | Epsilon: 0.781\n",
            "Episode  245 | Reward:  -69.07 | Avg(20):  -87.03 | Steps:   23118 | Epsilon: 0.780\n",
            "Episode  246 | Reward:  -75.10 | Avg(20):  -86.17 | Steps:   23240 | Epsilon: 0.779\n",
            "Episode  247 | Reward: -123.09 | Avg(20):  -88.19 | Steps:   23348 | Epsilon: 0.778\n",
            "Episode  248 | Reward: -103.46 | Avg(20):  -83.24 | Steps:   23423 | Epsilon: 0.777\n",
            "Episode  249 | Reward:  -95.28 | Avg(20):  -82.34 | Steps:   23570 | Epsilon: 0.776\n",
            "Episode  250 | Reward: -120.59 | Avg(20):  -82.10 | Steps:   23712 | Epsilon: 0.775\n",
            "Episode  251 | Reward:  -86.34 | Avg(20):  -78.24 | Steps:   23836 | Epsilon: 0.774\n",
            "Episode  252 | Reward:  -69.55 | Avg(20):  -77.59 | Steps:   23900 | Epsilon: 0.773\n",
            "Episode  253 | Reward:  -95.94 | Avg(20):  -79.43 | Steps:   23978 | Epsilon: 0.772\n",
            "Episode  254 | Reward:  -55.85 | Avg(20):  -78.92 | Steps:   24124 | Epsilon: 0.771\n",
            "Episode  255 | Reward:  -65.89 | Avg(20):  -78.28 | Steps:   24211 | Epsilon: 0.770\n",
            "Episode  256 | Reward:  -33.55 | Avg(20):  -73.27 | Steps:   24300 | Epsilon: 0.769\n",
            "Episode  257 | Reward: -159.34 | Avg(20):  -76.92 | Steps:   24398 | Epsilon: 0.768\n",
            "Episode  258 | Reward:  -57.64 | Avg(20):  -74.88 | Steps:   24505 | Epsilon: 0.767\n",
            "Episode  259 | Reward:  -90.64 | Avg(20):  -80.00 | Steps:   24587 | Epsilon: 0.766\n",
            "Episode  260 | Reward:  -99.59 | Avg(20):  -81.17 | Steps:   24687 | Epsilon: 0.765\n",
            "Episode  261 | Reward:  -53.82 | Avg(20):  -82.09 | Steps:   24801 | Epsilon: 0.764\n",
            "Episode  262 | Reward:    4.60 | Avg(20):  -78.80 | Steps:   24913 | Epsilon: 0.763\n",
            "Episode  263 | Reward:  -73.16 | Avg(20):  -79.81 | Steps:   25004 | Epsilon: 0.762\n",
            "Episode  264 | Reward:  -74.42 | Avg(20):  -79.89 | Steps:   25104 | Epsilon: 0.762\n",
            "Episode  265 | Reward:  -96.75 | Avg(20):  -81.27 | Steps:   25178 | Epsilon: 0.761\n",
            "Episode  266 | Reward:  -84.53 | Avg(20):  -81.74 | Steps:   25253 | Epsilon: 0.760\n",
            "Episode  267 | Reward:  -88.30 | Avg(20):  -80.00 | Steps:   25381 | Epsilon: 0.759\n",
            "Episode  268 | Reward:  -86.88 | Avg(20):  -79.17 | Steps:   25493 | Epsilon: 0.758\n",
            "Episode  269 | Reward:  -57.59 | Avg(20):  -77.29 | Steps:   25638 | Epsilon: 0.756\n",
            "Episode  270 | Reward:  -49.59 | Avg(20):  -73.74 | Steps:   25722 | Epsilon: 0.756\n",
            "Episode  271 | Reward:  -98.57 | Avg(20):  -74.35 | Steps:   25863 | Epsilon: 0.754\n",
            "Episode  272 | Reward:  -47.84 | Avg(20):  -73.26 | Steps:   25984 | Epsilon: 0.753\n",
            "Episode  273 | Reward: -115.84 | Avg(20):  -74.26 | Steps:   26093 | Epsilon: 0.752\n",
            "Episode  274 | Reward:  -87.41 | Avg(20):  -75.84 | Steps:   26171 | Epsilon: 0.751\n",
            "Episode  275 | Reward: -115.26 | Avg(20):  -78.31 | Steps:   26308 | Epsilon: 0.750\n",
            "Episode  276 | Reward:  -85.57 | Avg(20):  -80.91 | Steps:   26428 | Epsilon: 0.749\n",
            "Episode  277 | Reward:  -82.44 | Avg(20):  -77.06 | Steps:   26529 | Epsilon: 0.748\n",
            "Episode  278 | Reward:  -46.39 | Avg(20):  -76.50 | Steps:   26651 | Epsilon: 0.747\n",
            "Episode  279 | Reward:  -59.78 | Avg(20):  -74.96 | Steps:   26731 | Epsilon: 0.746\n",
            "Episode  280 | Reward: -123.89 | Avg(20):  -76.17 | Steps:   26838 | Epsilon: 0.745\n",
            "Episode  281 | Reward:  -88.16 | Avg(20):  -77.89 | Steps:   26967 | Epsilon: 0.744\n",
            "Episode  282 | Reward:  -17.25 | Avg(20):  -78.98 | Steps:   27068 | Epsilon: 0.743\n",
            "Episode  283 | Reward:  -96.36 | Avg(20):  -80.14 | Steps:   27167 | Epsilon: 0.742\n",
            "Episode  284 | Reward:  -53.31 | Avg(20):  -79.09 | Steps:   27235 | Epsilon: 0.741\n",
            "Episode  285 | Reward:  -34.88 | Avg(20):  -75.99 | Steps:   27368 | Epsilon: 0.740\n",
            "Episode  286 | Reward:  -67.56 | Avg(20):  -75.14 | Steps:   27512 | Epsilon: 0.739\n",
            "Episode  287 | Reward:  -70.72 | Avg(20):  -74.26 | Steps:   27577 | Epsilon: 0.738\n",
            "Episode  288 | Reward:  -69.59 | Avg(20):  -73.40 | Steps:   27732 | Epsilon: 0.737\n",
            "Episode  289 | Reward:  -38.55 | Avg(20):  -72.45 | Steps:   27829 | Epsilon: 0.736\n",
            "Episode  290 | Reward:  -70.89 | Avg(20):  -73.51 | Steps:   27966 | Epsilon: 0.734\n",
            "Episode  291 | Reward: -108.76 | Avg(20):  -74.02 | Steps:   28056 | Epsilon: 0.733\n",
            "Episode  292 | Reward:  -33.36 | Avg(20):  -73.30 | Steps:   28197 | Epsilon: 0.732\n",
            "Episode  293 | Reward:  -39.00 | Avg(20):  -69.46 | Steps:   28329 | Epsilon: 0.731\n",
            "Episode  294 | Reward:  -69.40 | Avg(20):  -68.56 | Steps:   28478 | Epsilon: 0.729\n",
            "Episode  295 | Reward:  -68.91 | Avg(20):  -66.24 | Steps:   28610 | Epsilon: 0.728\n",
            "Episode  296 | Reward: -117.52 | Avg(20):  -67.84 | Steps:   28709 | Epsilon: 0.727\n",
            "Episode  297 | Reward:  -29.12 | Avg(20):  -65.17 | Steps:   28834 | Epsilon: 0.726\n",
            "Episode  298 | Reward:  -53.14 | Avg(20):  -65.51 | Steps:   28924 | Epsilon: 0.725\n",
            "Episode  299 | Reward:  -37.69 | Avg(20):  -64.40 | Steps:   29069 | Epsilon: 0.724\n",
            "Episode  300 | Reward: -211.41 | Avg(20):  -68.78 | Steps:   29178 | Epsilon: 0.723\n",
            "Episode  301 | Reward:  -70.27 | Avg(20):  -67.88 | Steps:   29303 | Epsilon: 0.722\n",
            "Episode  302 | Reward: -152.35 | Avg(20):  -74.64 | Steps:   29593 | Epsilon: 0.719\n",
            "Episode  303 | Reward:  -18.81 | Avg(20):  -70.76 | Steps:   29684 | Epsilon: 0.718\n",
            "Episode  304 | Reward: -107.57 | Avg(20):  -73.47 | Steps:   29771 | Epsilon: 0.717\n",
            "Episode  305 | Reward:  -85.94 | Avg(20):  -76.03 | Steps:   29870 | Epsilon: 0.716\n",
            "Episode  306 | Reward:  -45.38 | Avg(20):  -74.92 | Steps:   29964 | Epsilon: 0.715\n",
            "Episode  307 | Reward:  -57.61 | Avg(20):  -74.26 | Steps:   30106 | Epsilon: 0.714\n",
            "Episode  308 | Reward:  -38.38 | Avg(20):  -72.70 | Steps:   30255 | Epsilon: 0.713\n",
            "Episode  309 | Reward:  -74.14 | Avg(20):  -74.48 | Steps:   30330 | Epsilon: 0.712\n",
            "Episode  310 | Reward:  -29.00 | Avg(20):  -72.39 | Steps:   30445 | Epsilon: 0.711\n",
            "Episode  311 | Reward:  -79.41 | Avg(20):  -70.92 | Steps:   30552 | Epsilon: 0.710\n",
            "Episode  312 | Reward:  -83.04 | Avg(20):  -73.40 | Steps:   30638 | Epsilon: 0.709\n",
            "Episode  313 | Reward: -118.83 | Avg(20):  -77.40 | Steps:   30720 | Epsilon: 0.708\n",
            "Episode  314 | Reward:  -80.65 | Avg(20):  -77.96 | Steps:   30829 | Epsilon: 0.707\n",
            "Episode  315 | Reward:  -60.13 | Avg(20):  -77.52 | Steps:   30932 | Epsilon: 0.706\n",
            "Episode  316 | Reward:  -71.10 | Avg(20):  -75.20 | Steps:   31062 | Epsilon: 0.705\n",
            "Episode  317 | Reward:  -69.42 | Avg(20):  -77.21 | Steps:   31186 | Epsilon: 0.704\n",
            "Episode  318 | Reward:  -83.32 | Avg(20):  -78.72 | Steps:   31294 | Epsilon: 0.703\n",
            "Episode  319 | Reward:  -65.94 | Avg(20):  -80.13 | Steps:   31368 | Epsilon: 0.702\n",
            "Episode  320 | Reward: -107.55 | Avg(20):  -74.94 | Steps:   31460 | Epsilon: 0.701\n",
            "Episode  321 | Reward:  -34.10 | Avg(20):  -73.13 | Steps:   31552 | Epsilon: 0.700\n",
            "Episode  322 | Reward:  -77.63 | Avg(20):  -69.40 | Steps:   31634 | Epsilon: 0.699\n",
            "Episode  323 | Reward:  -48.36 | Avg(20):  -70.88 | Steps:   31759 | Epsilon: 0.698\n",
            "Episode  324 | Reward:  -94.12 | Avg(20):  -70.20 | Steps:   31861 | Epsilon: 0.697\n",
            "Episode  325 | Reward:  -59.49 | Avg(20):  -68.88 | Steps:   31965 | Epsilon: 0.696\n",
            "Episode  326 | Reward:  -78.64 | Avg(20):  -70.54 | Steps:   32037 | Epsilon: 0.696\n",
            "Episode  327 | Reward:  -87.58 | Avg(20):  -72.04 | Steps:   32134 | Epsilon: 0.695\n",
            "Episode  328 | Reward:  -39.75 | Avg(20):  -72.11 | Steps:   32234 | Epsilon: 0.694\n",
            "Episode  329 | Reward:  -57.61 | Avg(20):  -71.28 | Steps:   32334 | Epsilon: 0.693\n",
            "Episode  330 | Reward:  -86.49 | Avg(20):  -74.16 | Steps:   32450 | Epsilon: 0.692\n",
            "Episode  331 | Reward:  -25.74 | Avg(20):  -71.48 | Steps:   32581 | Epsilon: 0.690\n",
            "Episode  332 | Reward:  -56.94 | Avg(20):  -70.17 | Steps:   32688 | Epsilon: 0.689\n",
            "Episode  333 | Reward:  -36.55 | Avg(20):  -66.06 | Steps:   32778 | Epsilon: 0.689\n",
            "Episode  334 | Reward:  -55.15 | Avg(20):  -64.78 | Steps:   32853 | Epsilon: 0.688\n",
            "Episode  335 | Reward:  -60.36 | Avg(20):  -64.79 | Steps:   32974 | Epsilon: 0.687\n",
            "Episode  336 | Reward:  -82.72 | Avg(20):  -65.37 | Steps:   33067 | Epsilon: 0.686\n",
            "Episode  337 | Reward:  -76.88 | Avg(20):  -65.75 | Steps:   33199 | Epsilon: 0.685\n",
            "Episode  338 | Reward:  -20.52 | Avg(20):  -62.61 | Steps:   33287 | Epsilon: 0.684\n",
            "Episode  339 | Reward:  -17.12 | Avg(20):  -60.17 | Steps:   33422 | Epsilon: 0.682\n",
            "Episode  340 | Reward:  -20.89 | Avg(20):  -55.83 | Steps:   33512 | Epsilon: 0.682\n",
            "Episode  341 | Reward:   -8.50 | Avg(20):  -54.55 | Steps:   33638 | Epsilon: 0.680\n",
            "Episode  342 | Reward:  -84.84 | Avg(20):  -54.91 | Steps:   33711 | Epsilon: 0.680\n",
            "Episode  343 | Reward:  -92.68 | Avg(20):  -57.13 | Steps:   33851 | Epsilon: 0.678\n",
            "Episode  344 | Reward:  -77.44 | Avg(20):  -56.29 | Steps:   33914 | Epsilon: 0.678\n",
            "Episode  345 | Reward:  -81.91 | Avg(20):  -57.42 | Steps:   34011 | Epsilon: 0.677\n",
            "Episode  346 | Reward:  -78.19 | Avg(20):  -57.39 | Steps:   34131 | Epsilon: 0.676\n",
            "Episode  347 | Reward:  -68.62 | Avg(20):  -56.45 | Steps:   34234 | Epsilon: 0.675\n",
            "Episode  348 | Reward: -109.45 | Avg(20):  -59.93 | Steps:   34321 | Epsilon: 0.674\n",
            "Episode  349 | Reward:  -49.95 | Avg(20):  -59.55 | Steps:   34404 | Epsilon: 0.673\n",
            "Episode  350 | Reward:  -58.11 | Avg(20):  -58.13 | Steps:   34513 | Epsilon: 0.672\n",
            "Episode  351 | Reward:  -74.36 | Avg(20):  -60.56 | Steps:   34591 | Epsilon: 0.671\n",
            "Episode  352 | Reward:  -62.46 | Avg(20):  -60.83 | Steps:   34680 | Epsilon: 0.671\n",
            "Episode  353 | Reward:  -87.09 | Avg(20):  -63.36 | Steps:   34778 | Epsilon: 0.670\n",
            "Episode  354 | Reward:  -69.01 | Avg(20):  -64.05 | Steps:   34879 | Epsilon: 0.669\n",
            "Episode  355 | Reward:  -49.10 | Avg(20):  -63.49 | Steps:   34955 | Epsilon: 0.668\n",
            "Episode  356 | Reward:    6.58 | Avg(20):  -59.03 | Steps:   35106 | Epsilon: 0.666\n",
            "Episode  357 | Reward:  -97.80 | Avg(20):  -60.07 | Steps:   35237 | Epsilon: 0.665\n",
            "Episode  358 | Reward:  -41.02 | Avg(20):  -61.10 | Steps:   35327 | Epsilon: 0.664\n",
            "Episode  359 | Reward:  -83.96 | Avg(20):  -64.44 | Steps:   35414 | Epsilon: 0.664\n",
            "Episode  360 | Reward: -113.97 | Avg(20):  -69.09 | Steps:   35481 | Epsilon: 0.663\n",
            "Episode  361 | Reward:  -45.83 | Avg(20):  -70.96 | Steps:   35545 | Epsilon: 0.662\n",
            "Episode  362 | Reward:  -54.01 | Avg(20):  -69.42 | Steps:   35627 | Epsilon: 0.662\n",
            "Episode  363 | Reward:  -13.23 | Avg(20):  -65.45 | Steps:   35704 | Epsilon: 0.661\n",
            "Episode  364 | Reward:  -46.56 | Avg(20):  -63.90 | Steps:   35787 | Epsilon: 0.660\n",
            "Episode  365 | Reward:  -58.42 | Avg(20):  -62.73 | Steps:   35879 | Epsilon: 0.659\n",
            "Episode  366 | Reward:  -46.88 | Avg(20):  -61.16 | Steps:   35965 | Epsilon: 0.658\n",
            "Episode  367 | Reward:  -82.82 | Avg(20):  -61.87 | Steps:   36079 | Epsilon: 0.657\n",
            "Episode  368 | Reward:  -33.95 | Avg(20):  -58.10 | Steps:   36146 | Epsilon: 0.657\n",
            "Episode  369 | Reward:  -51.20 | Avg(20):  -58.16 | Steps:   36250 | Epsilon: 0.656\n",
            "Episode  370 | Reward:  -75.18 | Avg(20):  -59.01 | Steps:   36347 | Epsilon: 0.655\n",
            "Episode  371 | Reward:  -61.05 | Avg(20):  -58.35 | Steps:   36469 | Epsilon: 0.654\n",
            "Episode  372 | Reward:  -90.48 | Avg(20):  -59.75 | Steps:   36559 | Epsilon: 0.653\n",
            "Episode  373 | Reward:  -49.23 | Avg(20):  -57.86 | Steps:   36647 | Epsilon: 0.652\n",
            "Episode  374 | Reward:  -29.81 | Avg(20):  -55.89 | Steps:   36738 | Epsilon: 0.651\n",
            "Episode  375 | Reward:  -63.99 | Avg(20):  -56.64 | Steps:   36845 | Epsilon: 0.650\n",
            "Episode  376 | Reward:  -22.79 | Avg(20):  -58.11 | Steps:   36926 | Epsilon: 0.649\n",
            "Episode  377 | Reward:   -7.29 | Avg(20):  -53.58 | Steps:   37074 | Epsilon: 0.648\n",
            "Episode  378 | Reward: -163.70 | Avg(20):  -59.72 | Steps:   37168 | Epsilon: 0.647\n",
            "Episode  379 | Reward:  -37.72 | Avg(20):  -57.40 | Steps:   37304 | Epsilon: 0.646\n",
            "Episode  380 | Reward:  -59.71 | Avg(20):  -54.69 | Steps:   37386 | Epsilon: 0.645\n",
            "Episode  381 | Reward:  -62.98 | Avg(20):  -55.55 | Steps:   37475 | Epsilon: 0.644\n",
            "Episode  382 | Reward:  -46.91 | Avg(20):  -55.19 | Steps:   37581 | Epsilon: 0.643\n",
            "Episode  383 | Reward:  -67.73 | Avg(20):  -57.92 | Steps:   37645 | Epsilon: 0.642\n",
            "Episode  384 | Reward:  -92.16 | Avg(20):  -60.20 | Steps:   37758 | Epsilon: 0.641\n",
            "Episode  385 | Reward: -136.25 | Avg(20):  -64.09 | Steps:   37894 | Epsilon: 0.640\n",
            "Episode  386 | Reward:  -38.48 | Avg(20):  -63.67 | Steps:   38033 | Epsilon: 0.639\n",
            "Episode  387 | Reward:  -51.39 | Avg(20):  -62.10 | Steps:   38095 | Epsilon: 0.638\n",
            "Episode  388 | Reward:  -46.93 | Avg(20):  -62.75 | Steps:   38174 | Epsilon: 0.637\n",
            "Episode  389 | Reward:  -64.03 | Avg(20):  -63.39 | Steps:   38258 | Epsilon: 0.637\n",
            "Episode  390 | Reward:    5.64 | Avg(20):  -59.35 | Steps:   38333 | Epsilon: 0.636\n",
            "Episode  391 | Reward:  -50.69 | Avg(20):  -58.83 | Steps:   38401 | Epsilon: 0.635\n",
            "Episode  392 | Reward:  -61.23 | Avg(20):  -57.37 | Steps:   38523 | Epsilon: 0.634\n",
            "Episode  393 | Reward:  -40.92 | Avg(20):  -56.95 | Steps:   38619 | Epsilon: 0.633\n",
            "Episode  394 | Reward:  -32.72 | Avg(20):  -57.10 | Steps:   38704 | Epsilon: 0.632\n",
            "Episode  395 | Reward:  -69.22 | Avg(20):  -57.36 | Steps:   38831 | Epsilon: 0.631\n",
            "Episode  396 | Reward: -100.32 | Avg(20):  -61.24 | Steps:   38930 | Epsilon: 0.630\n",
            "Episode  397 | Reward:  -24.85 | Avg(20):  -62.11 | Steps:   39000 | Epsilon: 0.630\n",
            "Episode  398 | Reward:  -80.52 | Avg(20):  -57.96 | Steps:   39095 | Epsilon: 0.629\n",
            "Episode  399 | Reward: -123.78 | Avg(20):  -62.26 | Steps:   39263 | Epsilon: 0.627\n",
            "Episode  400 | Reward:  -21.37 | Avg(20):  -60.34 | Steps:   39393 | Epsilon: 0.626\n",
            "Episode  401 | Reward:  -56.86 | Avg(20):  -60.04 | Steps:   39473 | Epsilon: 0.625\n",
            "Episode  402 | Reward:  -34.86 | Avg(20):  -59.43 | Steps:   39571 | Epsilon: 0.624\n",
            "Episode  403 | Reward:  -62.15 | Avg(20):  -59.15 | Steps:   39633 | Epsilon: 0.623\n",
            "Episode  404 | Reward:  -36.36 | Avg(20):  -56.36 | Steps:   39745 | Epsilon: 0.622\n",
            "Episode  405 | Reward:  -28.42 | Avg(20):  -50.97 | Steps:   39830 | Epsilon: 0.622\n",
            "Episode  406 | Reward:  -75.88 | Avg(20):  -52.84 | Steps:   39900 | Epsilon: 0.621\n",
            "Episode  407 | Reward:   27.48 | Avg(20):  -48.90 | Steps:   40035 | Epsilon: 0.620\n",
            "Episode  408 | Reward:  -26.98 | Avg(20):  -47.90 | Steps:   40176 | Epsilon: 0.618\n",
            "Episode  409 | Reward:  -57.81 | Avg(20):  -47.59 | Steps:   40310 | Epsilon: 0.617\n",
            "Episode  410 | Reward:  -52.29 | Avg(20):  -50.49 | Steps:   40383 | Epsilon: 0.616\n",
            "Episode  411 | Reward:  -81.25 | Avg(20):  -52.02 | Steps:   40455 | Epsilon: 0.616\n",
            "Episode  412 | Reward:    2.77 | Avg(20):  -48.82 | Steps:   40557 | Epsilon: 0.615\n",
            "Episode  413 | Reward:  -77.06 | Avg(20):  -50.62 | Steps:   40663 | Epsilon: 0.614\n",
            "Episode  414 | Reward:   -4.98 | Avg(20):  -49.24 | Steps:   40742 | Epsilon: 0.613\n",
            "Episode  415 | Reward:  -66.38 | Avg(20):  -49.09 | Steps:   40846 | Epsilon: 0.612\n",
            "Episode  416 | Reward:  -29.95 | Avg(20):  -45.58 | Steps:   40958 | Epsilon: 0.611\n",
            "Episode  417 | Reward: -202.62 | Avg(20):  -54.46 | Steps:   41053 | Epsilon: 0.610\n",
            "Episode  418 | Reward:  -26.89 | Avg(20):  -51.78 | Steps:   41124 | Epsilon: 0.609\n",
            "Episode  419 | Reward:  -23.98 | Avg(20):  -46.79 | Steps:   41211 | Epsilon: 0.608\n",
            "Episode  420 | Reward:  -61.05 | Avg(20):  -48.78 | Steps:   41339 | Epsilon: 0.607\n",
            "Episode  421 | Reward:  -29.65 | Avg(20):  -47.42 | Steps:   41406 | Epsilon: 0.607\n",
            "Episode  422 | Reward:   22.05 | Avg(20):  -44.57 | Steps:   41529 | Epsilon: 0.605\n",
            "Episode  423 | Reward:  -36.95 | Avg(20):  -43.31 | Steps:   41643 | Epsilon: 0.604\n",
            "Episode  424 | Reward:  -51.60 | Avg(20):  -44.07 | Steps:   41750 | Epsilon: 0.603\n",
            "Episode  425 | Reward:  -32.25 | Avg(20):  -44.26 | Steps:   41828 | Epsilon: 0.603\n",
            "Episode  426 | Reward:  -12.55 | Avg(20):  -41.10 | Steps:   41938 | Epsilon: 0.602\n",
            "Episode  427 | Reward:  -64.37 | Avg(20):  -45.69 | Steps:   42239 | Epsilon: 0.599\n",
            "Episode  428 | Reward:  -40.77 | Avg(20):  -46.38 | Steps:   42313 | Epsilon: 0.598\n",
            "Episode  429 | Reward:  -19.47 | Avg(20):  -44.46 | Steps:   42433 | Epsilon: 0.597\n",
            "Episode  430 | Reward:  -51.61 | Avg(20):  -44.43 | Steps:   42528 | Epsilon: 0.596\n",
            "Episode  431 | Reward:  -57.09 | Avg(20):  -43.22 | Steps:   42635 | Epsilon: 0.595\n",
            "Episode  432 | Reward:   -5.12 | Avg(20):  -43.61 | Steps:   42717 | Epsilon: 0.594\n",
            "Episode  433 | Reward:  -52.73 | Avg(20):  -42.40 | Steps:   42829 | Epsilon: 0.593\n",
            "Episode  434 | Reward:  -39.03 | Avg(20):  -44.10 | Steps:   42908 | Epsilon: 0.592\n",
            "Episode  435 | Reward:  -47.90 | Avg(20):  -43.18 | Steps:   42990 | Epsilon: 0.592\n",
            "Episode  436 | Reward:  -56.75 | Avg(20):  -44.52 | Steps:   43071 | Epsilon: 0.591\n",
            "Episode  437 | Reward:  -48.49 | Avg(20):  -36.81 | Steps:   43184 | Epsilon: 0.590\n",
            "Episode  438 | Reward:  -42.43 | Avg(20):  -37.59 | Steps:   43257 | Epsilon: 0.589\n",
            "Episode  439 | Reward:  -33.59 | Avg(20):  -38.07 | Steps:   43372 | Epsilon: 0.588\n",
            "Episode  440 | Reward:  -22.33 | Avg(20):  -36.13 | Steps:   43486 | Epsilon: 0.587\n",
            "Episode  441 | Reward:    0.85 | Avg(20):  -34.61 | Steps:   43613 | Epsilon: 0.586\n",
            "Episode  442 | Reward:  -23.52 | Avg(20):  -36.88 | Steps:   43696 | Epsilon: 0.585\n",
            "Episode  443 | Reward:  -39.56 | Avg(20):  -37.02 | Steps:   43781 | Epsilon: 0.584\n",
            "Episode  444 | Reward:  -19.48 | Avg(20):  -35.41 | Steps:   43900 | Epsilon: 0.583\n",
            "Episode  445 | Reward:  -15.84 | Avg(20):  -34.59 | Steps:   44028 | Epsilon: 0.582\n",
            "Episode  446 | Reward:  -33.30 | Avg(20):  -35.63 | Steps:   44099 | Epsilon: 0.581\n",
            "Episode  447 | Reward:  -18.05 | Avg(20):  -33.31 | Steps:   44183 | Epsilon: 0.580\n",
            "Episode  448 | Reward:  -46.23 | Avg(20):  -33.58 | Steps:   44296 | Epsilon: 0.579\n",
            "Episode  449 | Reward:  -18.15 | Avg(20):  -33.52 | Steps:   44438 | Epsilon: 0.578\n",
            "Episode  450 | Reward:  -13.84 | Avg(20):  -31.63 | Steps:   45438 | Epsilon: 0.568\n",
            "Episode  451 | Reward:  -70.88 | Avg(20):  -32.32 | Steps:   45532 | Epsilon: 0.567\n",
            "Episode  452 | Reward:  -81.14 | Avg(20):  -36.12 | Steps:   45644 | Epsilon: 0.566\n",
            "Episode  453 | Reward:  -23.28 | Avg(20):  -34.65 | Steps:   45713 | Epsilon: 0.566\n",
            "Episode  454 | Reward:  -53.32 | Avg(20):  -35.36 | Steps:   45805 | Epsilon: 0.565\n",
            "Episode  455 | Reward: -111.60 | Avg(20):  -38.55 | Steps:   45907 | Epsilon: 0.564\n",
            "Episode  456 | Reward:  -37.88 | Avg(20):  -37.60 | Steps:   46001 | Epsilon: 0.563\n",
            "Episode  457 | Reward:  -23.38 | Avg(20):  -36.35 | Steps:   46096 | Epsilon: 0.562\n",
            "Episode  458 | Reward: -170.90 | Avg(20):  -42.77 | Steps:   46216 | Epsilon: 0.561\n",
            "Episode  459 | Reward:  -91.69 | Avg(20):  -45.68 | Steps:   46349 | Epsilon: 0.560\n",
            "Episode  460 | Reward:  -40.49 | Avg(20):  -46.59 | Steps:   46440 | Epsilon: 0.559\n",
            "Episode  461 | Reward:   23.56 | Avg(20):  -45.45 | Steps:   46561 | Epsilon: 0.558\n",
            "Episode  462 | Reward:  -60.64 | Avg(20):  -47.31 | Steps:   46639 | Epsilon: 0.557\n",
            "Episode  463 | Reward:   17.22 | Avg(20):  -44.47 | Steps:   46775 | Epsilon: 0.556\n",
            "Episode  464 | Reward: -137.58 | Avg(20):  -50.37 | Steps:   46876 | Epsilon: 0.555\n",
            "Episode  465 | Reward:  -64.45 | Avg(20):  -52.80 | Steps:   46954 | Epsilon: 0.554\n",
            "Episode  466 | Reward:   -9.40 | Avg(20):  -51.61 | Steps:   47047 | Epsilon: 0.553\n",
            "Episode  467 | Reward:  -17.47 | Avg(20):  -51.58 | Steps:   47172 | Epsilon: 0.552\n",
            "Episode  468 | Reward: -147.17 | Avg(20):  -56.63 | Steps:   47431 | Epsilon: 0.549\n",
            "Episode  469 | Reward:  -18.85 | Avg(20):  -56.66 | Steps:   47515 | Epsilon: 0.549\n",
            "Episode  470 | Reward:  -69.68 | Avg(20):  -59.45 | Steps:   47581 | Epsilon: 0.548\n",
            "Episode  471 | Reward:  -25.45 | Avg(20):  -57.18 | Steps:   47655 | Epsilon: 0.547\n",
            "Episode  472 | Reward:  -29.70 | Avg(20):  -54.61 | Steps:   47748 | Epsilon: 0.546\n",
            "Episode  473 | Reward:  -57.76 | Avg(20):  -56.33 | Steps:   47811 | Epsilon: 0.546\n",
            "Episode  474 | Reward:  -65.60 | Avg(20):  -56.95 | Steps:   47897 | Epsilon: 0.545\n",
            "Episode  475 | Reward:   -8.39 | Avg(20):  -51.79 | Steps:   48010 | Epsilon: 0.544\n",
            "Episode  476 | Reward:  -47.12 | Avg(20):  -52.25 | Steps:   48111 | Epsilon: 0.543\n",
            "Episode  477 | Reward:    4.26 | Avg(20):  -50.87 | Steps:   48227 | Epsilon: 0.542\n",
            "Episode  478 | Reward:  -59.50 | Avg(20):  -45.29 | Steps:   48314 | Epsilon: 0.541\n",
            "Episode  479 | Reward:  -59.55 | Avg(20):  -43.69 | Steps:   48394 | Epsilon: 0.540\n",
            "Episode  480 | Reward:  -43.48 | Avg(20):  -43.84 | Steps:   48494 | Epsilon: 0.539\n",
            "Episode  481 | Reward:   42.58 | Avg(20):  -42.89 | Steps:   48584 | Epsilon: 0.538\n",
            "Episode  482 | Reward:  -54.50 | Avg(20):  -42.58 | Steps:   48693 | Epsilon: 0.537\n",
            "Episode  483 | Reward:  -54.35 | Avg(20):  -46.16 | Steps:   48808 | Epsilon: 0.536\n",
            "Episode  484 | Reward:  -40.03 | Avg(20):  -41.28 | Steps:   48942 | Epsilon: 0.535\n",
            "Episode  485 | Reward:  -77.14 | Avg(20):  -41.92 | Steps:   49054 | Epsilon: 0.534\n",
            "Episode  486 | Reward:  -55.92 | Avg(20):  -44.24 | Steps:   49156 | Epsilon: 0.533\n",
            "Episode  487 | Reward:  -45.09 | Avg(20):  -45.62 | Steps:   49273 | Epsilon: 0.532\n",
            "Episode  488 | Reward:  -70.76 | Avg(20):  -41.80 | Steps:   49608 | Epsilon: 0.529\n",
            "Episode  489 | Reward:  -31.75 | Avg(20):  -42.45 | Steps:   49751 | Epsilon: 0.527\n",
            "Episode  490 | Reward:  -38.06 | Avg(20):  -40.87 | Steps:   49854 | Epsilon: 0.526\n",
            "Episode  491 | Reward:  -79.57 | Avg(20):  -43.57 | Steps:   49933 | Epsilon: 0.526\n",
            "Episode  492 | Reward:  -84.86 | Avg(20):  -46.33 | Steps:   50005 | Epsilon: 0.525\n",
            "Episode  493 | Reward:  -94.65 | Avg(20):  -48.17 | Steps:   50103 | Epsilon: 0.524\n",
            "Episode  494 | Reward:  -39.85 | Avg(20):  -46.89 | Steps:   50223 | Epsilon: 0.523\n",
            "Episode  495 | Reward:  -58.42 | Avg(20):  -49.39 | Steps:   50300 | Epsilon: 0.522\n",
            "Episode  496 | Reward:  -35.19 | Avg(20):  -48.79 | Steps:   50404 | Epsilon: 0.521\n",
            "Episode  497 | Reward:  -14.42 | Avg(20):  -49.73 | Steps:   50523 | Epsilon: 0.520\n",
            "Episode  498 | Reward:  -37.09 | Avg(20):  -48.61 | Steps:   50638 | Epsilon: 0.519\n",
            "Episode  499 | Reward:    6.69 | Avg(20):  -45.29 | Steps:   50736 | Epsilon: 0.518\n",
            "Episode  500 | Reward:  -36.10 | Avg(20):  -44.92 | Steps:   50842 | Epsilon: 0.517\n",
            "Episode  501 | Reward:  -43.99 | Avg(20):  -49.25 | Steps:   50913 | Epsilon: 0.516\n",
            "Episode  502 | Reward:  -27.22 | Avg(20):  -47.89 | Steps:   50999 | Epsilon: 0.516\n",
            "Episode  503 | Reward:  -49.83 | Avg(20):  -47.66 | Steps:   51100 | Epsilon: 0.515\n",
            "Episode  504 | Reward:    4.63 | Avg(20):  -45.43 | Steps:   51182 | Epsilon: 0.514\n",
            "Episode  505 | Reward:  -34.39 | Avg(20):  -43.29 | Steps:   51285 | Epsilon: 0.513\n",
            "Episode  506 | Reward:  -47.95 | Avg(20):  -42.89 | Steps:   51383 | Epsilon: 0.512\n",
            "Episode  507 | Reward:    7.12 | Avg(20):  -40.28 | Steps:   51513 | Epsilon: 0.511\n",
            "Episode  508 | Reward:    0.44 | Avg(20):  -36.72 | Steps:   51593 | Epsilon: 0.510\n",
            "Episode  509 | Reward:    5.72 | Avg(20):  -34.85 | Steps:   52593 | Epsilon: 0.500\n",
            "Episode  510 | Reward:  -51.47 | Avg(20):  -35.52 | Steps:   52704 | Epsilon: 0.499\n",
            "Episode  511 | Reward:  -15.61 | Avg(20):  -32.32 | Steps:   52802 | Epsilon: 0.498\n",
            "Episode  512 | Reward:  -18.42 | Avg(20):  -29.00 | Steps:   52922 | Epsilon: 0.497\n",
            "Episode  513 | Reward:  -72.25 | Avg(20):  -27.88 | Steps:   53055 | Epsilon: 0.496\n",
            "Episode  514 | Reward:  -38.07 | Avg(20):  -27.79 | Steps:   53167 | Epsilon: 0.495\n",
            "Episode  515 | Reward:  -48.55 | Avg(20):  -27.30 | Steps:   53385 | Epsilon: 0.493\n",
            "Episode  516 | Reward:  -48.54 | Avg(20):  -27.96 | Steps:   53486 | Epsilon: 0.492\n",
            "Episode  517 | Reward: -112.62 | Avg(20):  -32.87 | Steps:   53952 | Epsilon: 0.487\n",
            "Episode  518 | Reward:   23.34 | Avg(20):  -29.85 | Steps:   54082 | Epsilon: 0.486\n",
            "Episode  519 | Reward:    0.23 | Avg(20):  -30.18 | Steps:   54185 | Epsilon: 0.485\n",
            "Episode  520 | Reward:  -76.23 | Avg(20):  -32.18 | Steps:   54291 | Epsilon: 0.484\n",
            "Episode  521 | Reward:  -44.96 | Avg(20):  -32.23 | Steps:   54384 | Epsilon: 0.483\n",
            "Episode  522 | Reward:  -39.25 | Avg(20):  -32.83 | Steps:   54486 | Epsilon: 0.482\n",
            "Episode  523 | Reward:  -47.58 | Avg(20):  -32.72 | Steps:   54568 | Epsilon: 0.482\n",
            "Episode  524 | Reward:  -12.09 | Avg(20):  -33.56 | Steps:   54637 | Epsilon: 0.481\n",
            "Episode  525 | Reward:  -46.34 | Avg(20):  -34.15 | Steps:   54709 | Epsilon: 0.480\n",
            "Episode  526 | Reward: -243.31 | Avg(20):  -43.92 | Steps:   55446 | Epsilon: 0.473\n",
            "Episode  527 | Reward: -282.30 | Avg(20):  -58.39 | Steps:   56416 | Epsilon: 0.464\n",
            "Episode  528 | Reward:  -37.26 | Avg(20):  -60.28 | Steps:   56529 | Epsilon: 0.463\n",
            "Episode  529 | Reward:   20.62 | Avg(20):  -59.53 | Steps:   56623 | Epsilon: 0.462\n",
            "Episode  530 | Reward:    4.48 | Avg(20):  -56.74 | Steps:   56712 | Epsilon: 0.461\n",
            "Episode  531 | Reward:   12.79 | Avg(20):  -55.32 | Steps:   56837 | Epsilon: 0.460\n",
            "Episode  532 | Reward:  -42.23 | Avg(20):  -56.51 | Steps:   56940 | Epsilon: 0.459\n",
            "Episode  533 | Reward:  -14.79 | Avg(20):  -53.63 | Steps:   57070 | Epsilon: 0.458\n",
            "Episode  534 | Reward:   -4.21 | Avg(20):  -51.94 | Steps:   57171 | Epsilon: 0.457\n",
            "Episode  535 | Reward:  -12.99 | Avg(20):  -50.16 | Steps:   57258 | Epsilon: 0.456\n",
            "Episode  536 | Reward:  -23.84 | Avg(20):  -48.93 | Steps:   57332 | Epsilon: 0.455\n",
            "Episode  537 | Reward:  -58.92 | Avg(20):  -46.24 | Steps:   57439 | Epsilon: 0.454\n",
            "Episode  538 | Reward:  -41.52 | Avg(20):  -49.48 | Steps:   57544 | Epsilon: 0.453\n",
            "Episode  539 | Reward:  -73.29 | Avg(20):  -53.16 | Steps:   57683 | Epsilon: 0.452\n",
            "Episode  540 | Reward:  -16.11 | Avg(20):  -50.15 | Steps:   57765 | Epsilon: 0.451\n",
            "Episode  541 | Reward:  -54.77 | Avg(20):  -50.65 | Steps:   57839 | Epsilon: 0.451\n",
            "Episode  542 | Reward:  -34.90 | Avg(20):  -50.43 | Steps:   58839 | Epsilon: 0.441\n",
            "Episode  543 | Reward:    9.46 | Avg(20):  -47.58 | Steps:   58913 | Epsilon: 0.440\n",
            "Episode  544 | Reward:  -67.33 | Avg(20):  -50.34 | Steps:   59041 | Epsilon: 0.439\n",
            "Episode  545 | Reward:   -1.94 | Avg(20):  -48.12 | Steps:   59135 | Epsilon: 0.438\n",
            "Episode  546 | Reward:  -41.84 | Avg(20):  -38.04 | Steps:   59253 | Epsilon: 0.437\n",
            "Episode  547 | Reward:  -67.74 | Avg(20):  -27.32 | Steps:   60253 | Epsilon: 0.428\n",
            "Episode  548 | Reward:  -36.15 | Avg(20):  -27.26 | Steps:   60357 | Epsilon: 0.427\n",
            "Episode  549 | Reward:  -81.59 | Avg(20):  -32.37 | Steps:   60526 | Epsilon: 0.425\n",
            "Episode  550 | Reward: -143.87 | Avg(20):  -39.79 | Steps:   60669 | Epsilon: 0.424\n",
            "Episode  551 | Reward:   40.98 | Avg(20):  -38.38 | Steps:   60754 | Epsilon: 0.423\n",
            "Episode  552 | Reward:    3.50 | Avg(20):  -36.09 | Steps:   60884 | Epsilon: 0.422\n",
            "Episode  553 | Reward:  -98.98 | Avg(20):  -40.30 | Steps:   61556 | Epsilon: 0.415\n",
            "Episode  554 | Reward:   -1.57 | Avg(20):  -40.17 | Steps:   61638 | Epsilon: 0.414\n",
            "Episode  555 | Reward:   -4.21 | Avg(20):  -39.73 | Steps:   61733 | Epsilon: 0.414\n",
            "Episode  556 | Reward:   -1.21 | Avg(20):  -38.60 | Steps:   61863 | Epsilon: 0.412\n",
            "Episode  557 | Reward:    3.26 | Avg(20):  -35.49 | Steps:   61975 | Epsilon: 0.411\n",
            "Episode  558 | Reward:   20.01 | Avg(20):  -32.42 | Steps:   62097 | Epsilon: 0.410\n",
            "Episode  559 | Reward:  -19.92 | Avg(20):  -29.75 | Steps:   62201 | Epsilon: 0.409\n",
            "Episode  560 | Reward:   10.64 | Avg(20):  -28.41 | Steps:   62354 | Epsilon: 0.408\n",
            "Episode  561 | Reward:    2.22 | Avg(20):  -25.56 | Steps:   62508 | Epsilon: 0.406\n",
            "Episode  562 | Reward:   14.17 | Avg(20):  -23.11 | Steps:   62642 | Epsilon: 0.405\n",
            "Episode  563 | Reward: -184.77 | Avg(20):  -32.82 | Steps:   62882 | Epsilon: 0.403\n",
            "Episode  564 | Reward: -292.36 | Avg(20):  -44.07 | Steps:   63033 | Epsilon: 0.401\n",
            "Episode  565 | Reward:   18.90 | Avg(20):  -43.03 | Steps:   63139 | Epsilon: 0.400\n",
            "Episode  566 | Reward:  -41.20 | Avg(20):  -43.00 | Steps:   63237 | Epsilon: 0.399\n",
            "Episode  567 | Reward:  -55.44 | Avg(20):  -42.38 | Steps:   63332 | Epsilon: 0.398\n",
            "Episode  568 | Reward:  -52.26 | Avg(20):  -43.19 | Steps:   63437 | Epsilon: 0.397\n",
            "Episode  569 | Reward:   -3.01 | Avg(20):  -39.26 | Steps:   64437 | Epsilon: 0.388\n",
            "Episode  570 | Reward:   -8.54 | Avg(20):  -32.49 | Steps:   65437 | Epsilon: 0.378\n",
            "Episode  571 | Reward:  -84.91 | Avg(20):  -38.79 | Steps:   66437 | Epsilon: 0.369\n",
            "Episode  572 | Reward:   33.19 | Avg(20):  -37.30 | Steps:   66545 | Epsilon: 0.368\n",
            "Episode  573 | Reward:   35.09 | Avg(20):  -30.60 | Steps:   66639 | Epsilon: 0.367\n",
            "Episode  574 | Reward:  -20.36 | Avg(20):  -31.54 | Steps:   66760 | Epsilon: 0.366\n",
            "Episode  575 | Reward:   21.68 | Avg(20):  -30.24 | Steps:   66909 | Epsilon: 0.364\n",
            "Episode  576 | Reward:    2.43 | Avg(20):  -30.06 | Steps:   67909 | Epsilon: 0.355\n",
            "Episode  577 | Reward:   57.01 | Avg(20):  -27.37 | Steps:   68000 | Epsilon: 0.354\n",
            "Episode  578 | Reward:   19.92 | Avg(20):  -27.38 | Steps:   68096 | Epsilon: 0.353\n",
            "Episode  579 | Reward:  -34.42 | Avg(20):  -28.10 | Steps:   69096 | Epsilon: 0.344\n",
            "Episode  580 | Reward:  -39.48 | Avg(20):  -30.61 | Steps:   69200 | Epsilon: 0.343\n",
            "Episode  581 | Reward:  -51.56 | Avg(20):  -33.30 | Steps:   70200 | Epsilon: 0.333\n",
            "Episode  582 | Reward:  -12.46 | Avg(20):  -34.63 | Steps:   70278 | Epsilon: 0.332\n",
            "Episode  583 | Reward:  -17.49 | Avg(20):  -26.26 | Steps:   71278 | Epsilon: 0.323\n",
            "Episode  584 | Reward:   -3.61 | Avg(20):  -11.83 | Steps:   72278 | Epsilon: 0.313\n",
            "Episode  585 | Reward:  -54.53 | Avg(20):  -15.50 | Steps:   72362 | Epsilon: 0.313\n",
            "Episode  586 | Reward:    0.29 | Avg(20):  -13.42 | Steps:   73362 | Epsilon: 0.303\n",
            "Episode  587 | Reward:    3.07 | Avg(20):  -10.50 | Steps:   74362 | Epsilon: 0.294\n",
            "Episode  588 | Reward:   32.87 | Avg(20):   -6.24 | Steps:   74450 | Epsilon: 0.293\n",
            "Episode  589 | Reward:    7.19 | Avg(20):   -5.73 | Steps:   75450 | Epsilon: 0.283\n",
            "Episode  590 | Reward:   59.21 | Avg(20):   -2.34 | Steps:   75546 | Epsilon: 0.282\n",
            "Episode  591 | Reward:  -44.21 | Avg(20):   -0.31 | Steps:   76546 | Epsilon: 0.273\n",
            "Episode  592 | Reward:   18.04 | Avg(20):   -1.07 | Steps:   76659 | Epsilon: 0.272\n",
            "Episode  593 | Reward:  -15.93 | Avg(20):   -3.62 | Steps:   77659 | Epsilon: 0.262\n",
            "Episode  594 | Reward:   17.91 | Avg(20):   -1.70 | Steps:   78659 | Epsilon: 0.253\n",
            "Episode  595 | Reward:  -47.10 | Avg(20):   -5.14 | Steps:   79659 | Epsilon: 0.243\n",
            "Episode  596 | Reward:  -28.81 | Avg(20):   -6.70 | Steps:   80659 | Epsilon: 0.234\n",
            "Episode  597 | Reward:  -36.10 | Avg(20):  -11.36 | Steps:   81659 | Epsilon: 0.224\n",
            "Episode  598 | Reward:  -35.45 | Avg(20):  -14.13 | Steps:   82659 | Epsilon: 0.215\n",
            "Episode  599 | Reward:   30.39 | Avg(20):  -10.89 | Steps:   82774 | Epsilon: 0.214\n",
            "Episode  600 | Reward:   -7.78 | Avg(20):   -9.30 | Steps:   83774 | Epsilon: 0.204\n",
            "Episode  601 | Reward:  -25.19 | Avg(20):   -7.98 | Steps:   84774 | Epsilon: 0.195\n",
            "Episode  602 | Reward:  -12.46 | Avg(20):   -7.98 | Steps:   85774 | Epsilon: 0.185\n",
            "Episode  603 | Reward:  -22.64 | Avg(20):   -8.24 | Steps:   86774 | Epsilon: 0.176\n",
            "Episode  604 | Reward:   -1.90 | Avg(20):   -8.16 | Steps:   87774 | Epsilon: 0.166\n",
            "Episode  605 | Reward:   -1.37 | Avg(20):   -5.50 | Steps:   88774 | Epsilon: 0.157\n",
            "Episode  606 | Reward:    4.60 | Avg(20):   -5.28 | Steps:   89774 | Epsilon: 0.147\n",
            "Episode  607 | Reward:  -28.49 | Avg(20):   -6.86 | Steps:   90774 | Epsilon: 0.138\n",
            "Episode  608 | Reward:  -25.85 | Avg(20):   -9.80 | Steps:   91774 | Epsilon: 0.128\n",
            "Episode  609 | Reward:  -24.65 | Avg(20):  -11.39 | Steps:   92774 | Epsilon: 0.119\n",
            "Episode  610 | Reward:    9.85 | Avg(20):  -13.86 | Steps:   93774 | Epsilon: 0.109\n",
            "Episode  611 | Reward:  -67.33 | Avg(20):  -15.01 | Steps:   94774 | Epsilon: 0.100\n",
            "Episode  612 | Reward:  -28.03 | Avg(20):  -17.32 | Steps:   95774 | Epsilon: 0.090\n",
            "Episode  613 | Reward:  -30.14 | Avg(20):  -18.03 | Steps:   96774 | Epsilon: 0.081\n",
            "Episode  614 | Reward:  -43.66 | Avg(20):  -21.11 | Steps:   97774 | Epsilon: 0.071\n",
            "Episode  615 | Reward:  -27.11 | Avg(20):  -20.11 | Steps:   98774 | Epsilon: 0.062\n",
            "Episode  616 | Reward:   10.75 | Avg(20):  -18.13 | Steps:   99774 | Epsilon: 0.052\n",
            "Episode  617 | Reward:    0.90 | Avg(20):  -16.28 | Steps:  100774 | Epsilon: 0.050\n",
            "Episode  618 | Reward:   18.78 | Avg(20):  -13.57 | Steps:  101774 | Epsilon: 0.050\n",
            "Episode  619 | Reward:   -7.98 | Avg(20):  -15.49 | Steps:  102774 | Epsilon: 0.050\n",
            "Episode  620 | Reward:  -99.67 | Avg(20):  -20.08 | Steps:  103774 | Epsilon: 0.050\n",
            "Episode  621 | Reward:   84.03 | Avg(20):  -14.62 | Steps:  104689 | Epsilon: 0.050\n",
            "Episode  622 | Reward:   -6.58 | Avg(20):  -14.32 | Steps:  105689 | Epsilon: 0.050\n",
            "Episode  623 | Reward:  -43.92 | Avg(20):  -15.39 | Steps:  106689 | Epsilon: 0.050\n",
            "Episode  624 | Reward:  -58.82 | Avg(20):  -18.23 | Steps:  107689 | Epsilon: 0.050\n",
            "Episode  625 | Reward: -123.53 | Avg(20):  -24.34 | Steps:  108689 | Epsilon: 0.050\n",
            "Episode  626 | Reward:  -41.81 | Avg(20):  -26.66 | Steps:  109689 | Epsilon: 0.050\n",
            "Episode  627 | Reward:   -0.60 | Avg(20):  -25.27 | Steps:  110689 | Epsilon: 0.050\n",
            "Episode  628 | Reward:  -75.37 | Avg(20):  -27.75 | Steps:  111689 | Epsilon: 0.050\n",
            "Episode  629 | Reward:  231.71 | Avg(20):  -14.93 | Steps:  112210 | Epsilon: 0.050\n",
            "Episode  630 | Reward:  -67.95 | Avg(20):  -18.82 | Steps:  113210 | Epsilon: 0.050\n",
            "Episode  631 | Reward:  -12.36 | Avg(20):  -16.07 | Steps:  114210 | Epsilon: 0.050\n",
            "Episode  632 | Reward:  146.52 | Avg(20):   -7.34 | Steps:  115028 | Epsilon: 0.050\n",
            "Episode  633 | Reward:  247.95 | Avg(20):    6.56 | Steps:  115508 | Epsilon: 0.050\n",
            "Episode  634 | Reward:  113.21 | Avg(20):   14.41 | Steps:  116508 | Epsilon: 0.050\n",
            "Episode  635 | Reward:  126.53 | Avg(20):   22.09 | Steps:  117508 | Epsilon: 0.050\n",
            "Episode  636 | Reward:  264.55 | Avg(20):   34.78 | Steps:  118033 | Epsilon: 0.050\n",
            "Episode  637 | Reward:  242.25 | Avg(20):   46.85 | Steps:  118722 | Epsilon: 0.050\n",
            "Episode  638 | Reward:  242.59 | Avg(20):   58.04 | Steps:  119182 | Epsilon: 0.050\n",
            "Episode  639 | Reward:  239.21 | Avg(20):   70.40 | Steps:  119548 | Epsilon: 0.050\n",
            "Episode  640 | Reward:  252.75 | Avg(20):   88.02 | Steps:  120057 | Epsilon: 0.050\n",
            "Episode  641 | Reward:  -54.27 | Avg(20):   81.10 | Steps:  121057 | Epsilon: 0.050\n",
            "Episode  642 | Reward:  263.05 | Avg(20):   94.58 | Steps:  121451 | Epsilon: 0.050\n",
            "Episode  643 | Reward:  233.52 | Avg(20):  108.46 | Steps:  121988 | Epsilon: 0.050\n",
            "Episode  644 | Reward:  271.22 | Avg(20):  124.96 | Steps:  122347 | Epsilon: 0.050\n",
            "Episode  645 | Reward:  266.66 | Avg(20):  144.47 | Steps:  122781 | Epsilon: 0.050\n",
            "Episode  646 | Reward:  264.90 | Avg(20):  159.80 | Steps:  123068 | Epsilon: 0.050\n",
            "Episode  647 | Reward:  267.20 | Avg(20):  173.19 | Steps:  123340 | Epsilon: 0.050\n",
            "Episode  648 | Reward:  231.10 | Avg(20):  188.52 | Steps:  123680 | Epsilon: 0.050\n",
            "Episode  649 | Reward:  248.32 | Avg(20):  189.35 | Steps:  124064 | Epsilon: 0.050\n",
            "Episode  650 | Reward:  249.73 | Avg(20):  205.23 | Steps:  124792 | Epsilon: 0.050\n",
            "Environment solved, stopping training.\n",
            "\n",
            "Evaluation over 20 episodes:\n",
            "  Mean reward: 101.20\n",
            "  Min reward:  -163.30\n",
            "  Max reward:  284.38\n",
            "\n",
            "Video rollout episode 0: total reward = -5.14\n",
            "Video rollout episode 1: total reward = 248.23\n",
            "Video rollout episode 2: total reward = 87.69\n",
            "Video rollout episode 3: total reward = 215.31\n",
            "Video rollout episode 4: total reward = -31.15\n",
            "Video rollout episode 5: total reward = 115.27\n",
            "Video rollout episode 6: total reward = 284.38\n",
            "Video rollout episode 7: total reward = 67.37\n",
            "Video rollout episode 8: total reward = 96.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video rollout episode 9: total reward = 265.08\n",
            "\n",
            "Saving best episode (reward=284.38) to lunar_lander_dqn.mp4 with 651 frames...\n",
            "Video saved: lunar_lander_dqn.mp4\n"
          ]
        }
      ]
    }
  ]
}